# Neural Networks

## Transformer
Transformer is a type of neural network architecture used in natural language processing tasks, such as language translation and text classification. It quickly became popular due to its ability to process sequences of data more efficiently than traditional RNNs or CNNs.

One of the key features of the Transformer is its use of self-attention mechanisms, which allow the network to focus on different parts of the input sequence and calculate relationships between them in parallel. This makes the Transformer well suited to handling sequences of varying lengths, which is a common problem in natural language processing. Another important feature of the Transformer is its use of multi-head attention, which allows the network to attend to multiple parts of the input sequence simultaneously and then concatenate the results to produce a single output. This enables the network to learn more complex relationships between the elements in the input sequence.

Transformer is introduced by *Vaswani, Ashish, et al. "Attention is all you need." Advances in neural information processing systems 30 (2017).* 
Please refer to this paper for more detailed explanation.

## Author
Bogyeong Suh
